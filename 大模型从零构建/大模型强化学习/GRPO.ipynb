{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ded09e5",
   "metadata": {},
   "source": [
    "# GRPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb56e33",
   "metadata": {},
   "source": [
    "内容暂时不添加\n",
    "\n",
    "（建议了解了大模型强化学习的知识后再观看）视频一：https://www.bilibili.com/video/BV1pXA5eyEEg/?spm_id_from=333.337.search-card.all.click&vd_source=071b23b9c7175dbaf674c65294124341"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b9c2fd",
   "metadata": {},
   "source": [
    "## 问题\n",
    "1、为什么一开始loss为0？  \n",
    "（1）KL散度为0  \n",
    "在GRPO算法中，训练开始时，被训练模型（actor模型）的权重与参考模型（ref_model）一致；  \n",
    "（2）advantages为0  \n",
    "在训练初期，模型生成的回答通常较为相似，导致组内奖励值的差异较小。具体来说：如果所有生成的回答质量相近，组内奖励值的均值与组内每个回答的奖励值接近，标准差接近 0，计算出来的advantages为0；\n",
    "\n",
    "2、为什么随着训练loss开始上升？  \n",
    "一个最简单的解释：初始化时，policy和reference policy相同，loss为0，随着训练进行，policy偏离reference policy，loss上升。  \n",
    "\n",
    "3、为什么训练一段时间后loss开始下降？\n",
    "（1）KL散度的变化$^-^$还未完全理解；\n",
    "（2）优势函数：随着训练的进行，模型生成的回答质量逐渐提高，优势函数的值开始趋于为0，参考一开始loss为0时为什么advantages为0。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68720c09",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
