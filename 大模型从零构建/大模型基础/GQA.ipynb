{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d599271b",
   "metadata": {},
   "source": [
    "# GQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f80f85",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"GQA/GQA.png\" alt=\"图片描述\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a4ba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原多头注意力机制\n",
    "\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024   # 句子长度\n",
    "    vocab_size: int = 50257  # 字典大小\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_kv_head: int = 6  # 应用了GQA技术，kv的头数是q的一半\n",
    "    n_embd: int = 768\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        # att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        # att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        # att = F.softmax(att, dim=-1)\n",
    "        # y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        \n",
    "        # 将上面的几个步骤改为下面的一个步骤，即使用了flash-attention\n",
    "        y = F.scaled_dot_product_attention(q, k, v,is_causal=True)\n",
    "\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4ee5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"torch.repeat_interleave(x, dim=1, repeats=n_rep)\"\"\"\n",
    "    B, n_kv_head, T, head_dim = x.shape\n",
    "    # 根据n_rep，拓展KV\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return (x[:, :, None, :, :].expand(B, n_kv_head, n_rep, T, head_dim).reshape(B, n_kv_head * n_rep, T, head_dim))\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        assert config.n_head % config.n_kv_head == 0\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.n_kv_head = config.n_kv_head\n",
    "        self.head_dim = config.n_embd // config.n_head\n",
    "\n",
    "        # key, query, value projections for all heads\n",
    "        self.q_proj = nn.Linear(self.n_embd, self.n_head * self.head_dim)   # self.n_head * self.head_dim就等于self.n_embd\n",
    "        self.k_proj = nn.Linear(self.n_embd, self.n_kv_head * self.head_dim)\n",
    "        self.v_proj = nn.Linear(self.n_embd, self.n_kv_head * self.head_dim)\n",
    "        # output projection\n",
    "        self.o_proj = nn.Linear(self.n_head * self.head_dim, self.n_embd)\n",
    "\n",
    "        # 旋转位置编码\n",
    "        self.rotary = RotaryPositionEmbedding(self.head_dim)\n",
    "\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2) # (B, n_head, T, head_dim)\n",
    "        k = k.view(B, T, self.n_kv_head, self.head_dim).transpose(1, 2) # (B, n_kv_head, T, head_dim)\n",
    "        v = v.view(B, T, self.n_kv_head, self.head_dim).transpose(1, 2) # (B, n_kv_head, T, head_dim)\n",
    "\n",
    "        # 旋转位置编码\n",
    "        q = self.rotary(q)\n",
    "        k = self.rotary(k)\n",
    "\n",
    "        k = torch.repeat_interleave(k, dim=1, repeats=self.n_head // self.n_kv_head)\n",
    "        v = torch.repeat_interleave(v, dim=1, repeats=self.n_head // self.n_kv_head)\n",
    "        # k = repeat_kv(k, self.n_head // self.n_kv_head)\n",
    "        # v = repeat_kv(v, self.n_head // self.n_kv_head)\n",
    "\n",
    "        # # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        # att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        # att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        # att = F.softmax(att, dim=-1)\n",
    "        # y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        \n",
    "        # 将上面的几个步骤改为下面的一个步骤，即使用了flash-attention\n",
    "        y = F.scaled_dot_product_attention(q, k, v,is_causal=True)\n",
    "\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.o_proj(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acdeb4e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseek2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
