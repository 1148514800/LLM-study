{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 位置编码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学习顺序：\n",
    "1、位置编码介绍：https://www.bilibili.com/video/BV1xR1RY9ECm/?spm_id_from=333.337.search-card.all.click&vd_source=071b23b9c7175dbaf674c65294124341  \n",
    "2、transformer位置编码介绍：https://www.bilibili.com/video/BV1AD421g7hs/?spm_id_from=333.337.search-card.all.click&vd_source=071b23b9c7175dbaf674c65294124341  \n",
    "3、RoPE：视频一： https://www.bilibili.com/video/BV12x42127Pb?spm_id_from=333.788.videopod.sections&vd_source=071b23b9c7175dbaf674c65294124341  \n",
    "视频二：https://www.bilibili.com/video/BV1F1421B7iv/?spm_id_from=333.337.search-card.all.click&vd_source=071b23b9c7175dbaf674c65294124341\n",
    "  \n",
    "  \n",
    "\n",
    "优势：和相对位置编码相比，RoPE 具有更好的外推性，即对于超出训练数据长度的序列，RoPE 仍然能够提供有效的位置编码。这是因为 RoPE 的位置编码是基于正弦和余弦函数的周期性特性，而相对位置编码则依赖于训练数据中的相对位置关系。例如，如果一个模型在训练时只使用了512个 token 的文本，那么在预测时如果输入超过512个 token，模型可能无法正确处理。这就限制了大模型在处理长文本或多轮对话等任务时的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、学习三角式绝对位置编码  \n",
    "该方法无法进行学习与更新，位置写定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绝对位置编码，transformer中的三角式\n",
    "import math\n",
    "import torch\n",
    "\n",
    "seq_len = 50           # sequence 长度，block_size\n",
    "embedding_dim = 512    # 单个 token 的编码维度\n",
    "\n",
    "def get_pe(pos, j, dim):\n",
    "    pe = pos / (10000 ** (2 * j / dim))\n",
    "    return math.sin(pe), math.cos(pe)\n",
    "\n",
    "pe = torch.empty(seq_len, embedding_dim)\n",
    "for i in range(seq_len):\n",
    "    for j in range(0, embedding_dim // 2):\n",
    "        pe[i, 2*j], pe[i, 2*j+1] = get_pe(i, j, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、可学习式绝对位置编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "wpe=nn.Embedding(seq_len, embedding_dim)\n",
    "# wpe_para = nn.Parameter(torch.randn(seq_len, embedding_dim))\n",
    "pos = torch.arange(0, seq_len, dtype=torch.long) # shape (T)\n",
    "pos_emb = wpe(pos)\n",
    "# wpe_para = wpe[pos] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、旋转位置编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入形状: torch.Size([2, 10, 128])\n",
      "输出形状: torch.Size([2, 10, 64])\n",
      "输出示例:\n",
      "tensor([-0.1196, -0.7089,  0.0172, -0.8604, -0.4860, -0.5124, -0.6146, -0.3723,\n",
      "         0.3353,  0.0694], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 无多头注意力机制版\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.nn import functional as F\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"将输入张量的后半部分旋转\"\"\"\n",
    "    # x.chunk(2, dim=-1)为将最后一个维度切成两半，返回两个张量\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    # 等价于下面两行\n",
    "    # x1 = x[..., : x.shape[-1] // 2]\n",
    "    # x2 = x[..., x.shape[-1] // 2:]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, sin, cos):\n",
    "    \"\"\"应用旋转位置编码到查询和键向量\"\"\"\n",
    "    # cos = cos.unsqueeze(1)\n",
    "    # sin = sin.unsqueeze(1)\n",
    "    # 对查询向量应用旋转\n",
    "    q_rot = (q * cos) + (rotate_half(q) * sin)\n",
    "    # 对键向量应用旋转\n",
    "    k_rot = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_rot, k_rot\n",
    "\n",
    "class RotaryPositionEmbedding(nn.Module):\n",
    "    \"\"\"旋转位置编码模块\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "\n",
    "    def _get_sin_cos(self, seq_len, device):\n",
    "        pos = torch.arange(seq_len, device=device).type_as(self.inv_freq)   # 加上绝对位置编码信息\n",
    "        freqs = torch.einsum(\"i,j->ij\", pos, self.inv_freq) # shape (seq_len, dim//2)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        return emb.sin(), emb.cos()\n",
    "\n",
    "    def forward(self, q, k):\n",
    "        batch_size, seq_len, _, _ = q.size()\n",
    "        sin, cos = self._get_sin_cos(seq_len, q.device) # shape (seq_len, dim)\n",
    "        # sin[None, :, None, :]就等于sin.view(1,seq_len,1,dim)\n",
    "        sin = sin[None, :, None, :].expand(batch_size, -1, -1, -1)  # 添加batch_size维度\n",
    "        cos = cos[None, :, None, :].expand(batch_size, -1, -1, -1)\n",
    "        return apply_rotary_pos_emb(q, k, sin, cos)\n",
    "\n",
    "class AttentionWithRoPE(nn.Module):\n",
    "    \"\"\"使用旋转位置编码的自注意力模块\"\"\"\n",
    "    def __init__(self, dim, head_dim):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.Wq = nn.Linear(dim, head_dim, bias=False)\n",
    "        self.Wk = nn.Linear(dim, head_dim, bias=False)\n",
    "        self.Wv = nn.Linear(dim, head_dim, bias=False)\n",
    "        self.rotary = RotaryPositionEmbedding(head_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # 生成Q, K, V\n",
    "        q = self.Wq(x).view(batch_size, seq_len, 1, self.head_dim)\n",
    "        k = self.Wk(x).view(batch_size, seq_len, 1, self.head_dim)\n",
    "        v = self.Wv(x).view(batch_size, seq_len, 1, self.head_dim)\n",
    "        \n",
    "        # 应用旋转位置编码\n",
    "        q, k = self.rotary(q, k)\n",
    "        \n",
    "        # 计算注意力分数\n",
    "        scores = torch.einsum(\"bnid,bnjd->bnij\", q, k) / math.sqrt(self.head_dim)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        # 应用注意力到值向量\n",
    "        output = torch.einsum(\"bnij,bnjd->bnid\", attn, v)\n",
    "        output = output.view(batch_size, seq_len, self.head_dim)\n",
    "\n",
    "        # output = F.scaled_dot_product_attention(q, k, v,is_causal=True) # 使用 PyTorch 的内置函数计算注意力\n",
    "        # output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.head_dim)\n",
    "        return output\n",
    "\n",
    "# 示例使用\n",
    "if __name__ == \"__main__\":\n",
    "    dim = 128  # 模型维度\n",
    "    head_dim = 64  # 每个注意力头的维度\n",
    "    seq_len = 10  # 序列长度\n",
    "    batch_size = 2  # 批大小\n",
    "\n",
    "    # 初始化注意力模块\n",
    "    attn = AttentionWithRoPE(dim, head_dim)\n",
    "    \n",
    "    # 生成随机输入\n",
    "    x = torch.randn(batch_size, seq_len, dim)\n",
    "    \n",
    "    # 前向传播\n",
    "    output = attn(x)\n",
    "    \n",
    "    print(\"输入形状:\", x.shape)\n",
    "    print(\"输出形状:\", output.shape)\n",
    "    print(\"输出示例:\")\n",
    "    print(output[0, 0, :10])  # 打印第一个样本第一个位置的输出前10维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入形状: torch.Size([2, 10, 128])\n",
      "输出形状: torch.Size([2, 10, 128])\n",
      "输出示例:\n",
      "tensor([-0.0527, -0.1726,  0.0473,  0.1299,  0.0536, -0.1116, -0.1719,  0.0064,\n",
      "        -0.0302,  0.1980], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 有多头注意力机制版\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"将输入张量的后半部分旋转\"\"\"\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, sin, cos):\n",
    "    \"\"\"应用旋转位置编码到查询和键向量\"\"\"\n",
    "    q_rot = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_rot = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_rot, k_rot\n",
    "\n",
    "class RotaryPositionEmbedding(nn.Module):\n",
    "    \"\"\"旋转位置编码模块（支持多头）\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "\n",
    "    def _get_sin_cos(self, seq_len, device):\n",
    "        pos = torch.arange(seq_len, device=device).type_as(self.inv_freq)\n",
    "        freqs = torch.einsum(\"i,j->ij\", pos, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        return emb.sin(), emb.cos()\n",
    "\n",
    "    def forward(self, q, k):\n",
    "        batch_size, seq_len, num_heads, _ = q.size()\n",
    "        sin, cos = self._get_sin_cos(seq_len, q.device) # sin.shape = cos.shape = (seq_len, head_dim)\n",
    "        \n",
    "        # 扩展维度适配多头 [batch_size, seq_len, num_heads, head_dim]\n",
    "        sin = sin.view(1, seq_len, 1, -1).expand(batch_size, -1, num_heads, -1)\n",
    "        cos = cos.view(1, seq_len, 1, -1).expand(batch_size, -1, num_heads, -1)\n",
    "        \n",
    "        return apply_rotary_pos_emb(q, k, sin, cos)\n",
    "\n",
    "class MultiHeadAttentionWithRoPE(nn.Module):\n",
    "    \"\"\"集成4个头并支持旋转位置编码的自注意力模块\"\"\"\n",
    "    def __init__(self, dim, num_heads=4):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, \"dim必须能被num_heads整除\"\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "\n",
    "        # 初始化投影矩阵\n",
    "        self.Wq = nn.Linear(dim, dim, bias=False)\n",
    "        self.Wk = nn.Linear(dim, dim, bias=False)\n",
    "        self.Wv = nn.Linear(dim, dim, bias=False)\n",
    "        \n",
    "        # 旋转位置编码\n",
    "        self.rotary = RotaryPositionEmbedding(self.head_dim)\n",
    "        \n",
    "        # 输出投影\n",
    "        self.out_proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # 生成Q, K, V并分头\n",
    "        q = self.Wq(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        k = self.Wk(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        v = self.Wv(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # 应用旋转位置编码\n",
    "        q, k = self.rotary(q, k)\n",
    "        \n",
    "        # 调整维度顺序 [batch, heads, seq_len, head_dim]\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        \n",
    "        # 计算注意力分数\n",
    "        scores = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.head_dim)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        # 应用注意力到值向量\n",
    "        output = torch.matmul(attn, v)  # [batch, heads, seq_len, head_dim]\n",
    "        \n",
    "        # 合并多头输出\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        output = output.view(batch_size, seq_len, self.dim)\n",
    "        \n",
    "        # 最终投影\n",
    "        return self.out_proj(output)\n",
    "\n",
    "# 示例使用\n",
    "if __name__ == \"__main__\":\n",
    "    dim = 128    # 总维度\n",
    "    num_heads = 4 # 注意力头数\n",
    "    seq_len = 10  # 序列长度\n",
    "    batch_size = 2 # 批大小\n",
    "\n",
    "    # 初始化注意力模块\n",
    "    attn = MultiHeadAttentionWithRoPE(dim, num_heads)\n",
    "    \n",
    "    # 生成随机输入\n",
    "    x = torch.randn(batch_size, seq_len, dim)\n",
    "    \n",
    "    # 前向传播\n",
    "    output = attn(x)\n",
    "    \n",
    "    print(\"输入形状:\", x.shape)\n",
    "    print(\"输出形状:\", output.shape)\n",
    "    print(\"输出示例:\")\n",
    "    print(output[0, 0, :10])  # 打印第一个样本第一个位置的输出前10维"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseek2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
