{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e2ca19b",
   "metadata": {},
   "source": [
    "## LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17d8b6a",
   "metadata": {},
   "source": [
    "视频一：https://www.bilibili.com/video/BV1fHmkYyE2w/?spm_id_from=333.1007.top_right_bar_window_default_collection.content.click&vd_source=071b23b9c7175dbaf674c65294124341  \n",
    "视频二：https://www.bilibili.com/video/BV1YVpte7EFL/?spm_id_from=333.1007.top_right_bar_window_history.content.click&vd_source=071b23b9c7175dbaf674c65294124341  \n",
    "视频三：https://www.bilibili.com/video/BV1dr421w7J5/?spm_id_from=333.337.search-card.all.click&vd_source=071b23b9c7175dbaf674c65294124341\n",
    "\n",
    "博客一：https://zhuanlan.zhihu.com/p/658007966\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a64b8adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape (no merge): torch.Size([32, 128, 512])\n",
      "Output shape (merged): torch.Size([32, 128, 512])\n",
      "Max difference after merge/unmerge cycle: 0.0\n"
     ]
    }
   ],
   "source": [
    "# 视频一\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class LinearLoRALayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, r=0.5, lora_alpha=1, lora_dropout=0.0, merge=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.r = r\n",
    "        self.lora_alpha = lora_alpha\n",
    "        self.merge = merge\n",
    "        self.lora_dropout = lora_dropout\n",
    "\n",
    "        # linear: weight的shape为 [out_features, in_features]\n",
    "        # input x shape是 [batch_size, seq_len, in_features]\n",
    "        # 计算过程是 x @ weight.T\n",
    "        # 所以 weight 的shape 是 [out_features, in_features]\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "\n",
    "        if r > 0:\n",
    "            self.lora_a = nn.Parameter(torch.randn((out_features, r)))\n",
    "            # 高斯分布\n",
    "            nn.init.kaiming_uniform_(self.lora_a, a=math.sqrt(5))\n",
    "\n",
    "            self.lora_b = nn.Parameter(torch.randn((r, in_features)))\n",
    "            self.scale = lora_alpha / r\n",
    "\n",
    "            self.linear.weight.requires_grad = False\n",
    "\n",
    "        self.dropout = nn.Dropout(lora_dropout) if lora_dropout > 0 else nn.Identity()\n",
    "\n",
    "        if merge:\n",
    "            self.merge_weight()\n",
    "    \n",
    "    def merge_weight(self):\n",
    "        if self.merge and self.r > 0:\n",
    "            self.linear.weight.data += self.scale * (self.lora_a @ self.lora_b)\n",
    "    \n",
    "    def unmerge_weight(self):\n",
    "        if self.merge and self.r > 0:\n",
    "            self.linear.weight.data -= self.scale * (self.lora_a @ self.lora_b)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len, in_features]\n",
    "\n",
    "        if self.r > 0:\n",
    "            output_part1 = self.linear(x)\n",
    "            output_part2 = self.scale * (x @ (self.lora_a @ self.lora_b).T)\n",
    "            output = output_part1 + output_part2\n",
    "        else:\n",
    "            output = self.linear(x)\n",
    "\n",
    "        return self.dropout(output)\n",
    "    \n",
    "\n",
    "# 视频一\n",
    "\n",
    "# 写一段测试代码\n",
    "# Test the LoRALinear layer\n",
    "batch_size = 32\n",
    "seq_len = 128\n",
    "in_features = 768\n",
    "out_features = 512\n",
    "rank = 8\n",
    "lora_alpha = 16\n",
    "dropout = 0.1\n",
    "\n",
    "# Create a test input\n",
    "x = torch.randn(batch_size, seq_len, in_features)\n",
    "\n",
    "# Test regular mode (no merge)\n",
    "lora_layer = LinearLoRALayer(\n",
    "    in_features=in_features,\n",
    "    out_features=out_features,\n",
    "    r=rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    dropout=dropout,\n",
    "    merge=False\n",
    ")\n",
    "\n",
    "# Forward pass\n",
    "output = lora_layer(x)\n",
    "print(f\"Output shape (no merge): {output.shape}\")\n",
    "\n",
    "# Test merged mode\n",
    "lora_layer_merged = LinearLoRALayer(\n",
    "    in_features=in_features,\n",
    "    out_features=out_features,\n",
    "    r=rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    dropout=dropout,\n",
    "    merge=True\n",
    ")\n",
    "\n",
    "# Forward pass with merged weights\n",
    "output_merged = lora_layer_merged(x)\n",
    "print(f\"Output shape (merged): {output_merged.shape}\")\n",
    "\n",
    "# Test weight merging/unmerging\n",
    "lora_layer.merge_weight()\n",
    "output_after_merge = lora_layer(x)\n",
    "lora_layer.unmerge_weight()\n",
    "output_after_unmerge = lora_layer(x)\n",
    "\n",
    "print(\"Max difference after merge/unmerge cycle:\", \n",
    "      torch.max(torch.abs(output - output_after_unmerge)).item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ddecddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b317eea347694c3894f9dfa5da176f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 3584)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 视频二\n",
    "from transformers import AutoModelForCausalLM\n",
    "# Check for available GPU\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载本地模型和分词器\n",
    "model_path = \"/home/hpclp/disk/q/models/Qwen2.5-7B-Instruct\"  # 替换为你的本地模型路径\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89ee75d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db46a58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 视频二\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        self.W_a = nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
    "        self.W_b = nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x @ self.W_a @ self.W_b) * self.alpha\n",
    "        return x\n",
    "\n",
    "class LinearWithLoRA(nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(linear.in_features, linear.out_features, rank, alpha)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c84373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 视频二\n",
    "\n",
    "lora_rank = 8\n",
    "lora_alpha = 16\n",
    "lora_q = True\n",
    "lora_k = True\n",
    "lora_v = True\n",
    "lora_o = True\n",
    "lora_mlp = True\n",
    "\n",
    "# # Apply LoRA to the specified layers\n",
    "# for name, module in model.named_modules():\n",
    "#     print(name, module)\n",
    "\n",
    "for layer in model.model.layers:\n",
    "    # print(layer.self_attn.q_proj)\n",
    "    if lora_q:\n",
    "        layer.self_attn.q_proj = LinearWithLoRA(layer.self_attn.q_proj, lora_rank, lora_alpha)\n",
    "    if lora_k:\n",
    "        layer.self_attn.k_proj = LinearWithLoRA(layer.self_attn.k_proj, lora_rank, lora_alpha)\n",
    "    if lora_v:\n",
    "        layer.self_attn.v_proj = LinearWithLoRA(layer.self_attn.v_proj, lora_rank, lora_alpha)\n",
    "    if lora_o:\n",
    "        layer.self_attn.o_proj = LinearWithLoRA(layer.self_attn.o_proj, lora_rank, lora_alpha)\n",
    "    if lora_mlp:\n",
    "        # 替换MLP中的线性层\n",
    "        layer.mlp.gate_proj = LinearWithLoRA(layer.mlp.gate_proj, lora_rank, lora_alpha)\n",
    "        layer.mlp.up_proj = LinearWithLoRA(layer.mlp.up_proj, lora_rank, lora_alpha)\n",
    "        layer.mlp.down_proj = LinearWithLoRA(layer.mlp.down_proj, lora_rank, lora_alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35803cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只训练LoRA参数\n",
    "for name, param in model.named_parameters():\n",
    "    if 'lora' not in name:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffa99cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 3584)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "          (k_proj): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=3584, out_features=512, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "          (v_proj): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=3584, out_features=512, bias=True)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "          (o_proj): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "          (up_proj): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "          (down_proj): LinearWithLoRA(\n",
       "            (linear): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "            (lora): LoRALayer()\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cc6057",
   "metadata": {},
   "source": [
    "## 视频三"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21fd02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class LinearLoRALayer(nn.Module):\n",
    "    def __init__(self, linear, r=8, lora_alpha=16, lora_dropout=0.1, merge=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.r = r\n",
    "        self.lora_alpha = lora_alpha\n",
    "        self.merge = merge\n",
    "        self.lora_dropout = lora_dropout\n",
    "        self.linear1 = linear\n",
    "\n",
    "        # linear: weight的shape为 [out_features, in_features]\n",
    "        # input x shape是 [batch_size, seq_len, in_features]\n",
    "        # 计算过程是 x @ weight.T\n",
    "        # 所以 weight 的shape 是 [out_features, in_features]\n",
    "\n",
    "        if r > 0:\n",
    "            self.lora_a = nn.Parameter(torch.zeros((r, linear.in_features)))\n",
    "            self.lora_b = nn.Parameter(torch.zeros((linear.out_features, r)))\n",
    "            self.scale = lora_alpha / r\n",
    "            self.linear1.weight.requires_grad = False\n",
    "\n",
    "        self.dropout = nn.Dropout(lora_dropout) if lora_dropout > 0 else nn.Identity()\n",
    "\n",
    "        self.initial_weights()\n",
    "\n",
    "\n",
    "    def initial_weights(self):\n",
    "        nn.init.kaiming_normal_(self.lora_a, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_b)\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len, in_features]\n",
    "\n",
    "        if self.r > 0 and self.merge:\n",
    "            # output = F.linear(x, self.linear.weight, bias=self.linear.bias)\n",
    "            # output += (self.lora_dropout(x) @ (self.lora_b @ self.lora_a).T) * self.scale\n",
    "            output = F.linear(x, self.linear1.weight + self.scale * (self.lora_b @ self.lora_a), bias=self.linear1.bias)\n",
    "            output = self.dropout(output)\n",
    "            return output\n",
    "        else:\n",
    "            return self.dropout(self.linear1(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d08f5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # linear: weight的shape为 [out_features, in_features]\n",
    "        # input x shape是 [batch_size, seq_len, in_features]\n",
    "        # 计算过程是 x @ weight.T\n",
    "        # 所以 weight 的shape 是 [out_features, in_features]\n",
    "        self.lora_a = nn.Parameter(torch.zeros((rank, in_features)))\n",
    "        self.lora_b = nn.Parameter(torch.zeros((out_features, rank)))\n",
    "        self.scale = lora_alpha / rank\n",
    "\n",
    "\n",
    "        self.initial_weights()\n",
    "\n",
    "\n",
    "    def initial_weights(self):\n",
    "        nn.init.kaiming_normal_(self.lora_a, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_b)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.scale * (x @ (self.lora_b @ self.lora_a).T)\n",
    "\n",
    "\n",
    "class LinearLoRALayer(nn.Module):\n",
    "    def __init__(self, linear, r=8, lora_alpha=16, lora_dropout=0.1, merge=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.merge = merge\n",
    "        self.r = r\n",
    "        \n",
    "        if self.r > 0:\n",
    "            self.lora_layer = LoRALayer(linear.in_features, linear.out_features, r, lora_alpha)\n",
    "            self.linear.weight.requires_grad = False\n",
    "\n",
    "        self.dropout = nn.Dropout(lora_dropout) if lora_dropout > 0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len, in_features]\n",
    "\n",
    "        if self.r > 0 and self.merge:\n",
    "            output = F.linear(x, self.linear.weight, bias=self.linear.bias)     # 不会进行梯度更新\n",
    "            output += self.lora_layer(x)\n",
    "            return self.dropout(output)\n",
    "        else:\n",
    "            return self.linear(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e701a1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "# Check for available GPU\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载本地模型和分词器\n",
    "model_path = \"/home/hpclp/disk/q/models/Qwen2.5-0.5B-Instruct\"  # 替换为你的本地模型路径\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af15b763",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_rank = 8\n",
    "lora_alpha = 16\n",
    "lora_q = True\n",
    "lora_k = False\n",
    "lora_v = True\n",
    "lora_o = True\n",
    "lora_mlp = True\n",
    "\n",
    "# # Apply LoRA to the specified layers\n",
    "# for name, module in model.named_modules():\n",
    "#     print(name, module)\n",
    "\n",
    "for layer in model.model.layers:\n",
    "    # print(layer.self_attn.q_proj)\n",
    "    if lora_q:\n",
    "        layer.self_attn.q_proj = LinearLoRALayer(layer.self_attn.q_proj, lora_rank, lora_alpha)\n",
    "    if lora_k:\n",
    "        layer.self_attn.k_proj = LinearLoRALayer(layer.self_attn.k_proj, lora_rank, lora_alpha)\n",
    "    if lora_v:\n",
    "        layer.self_attn.v_proj = LinearLoRALayer(layer.self_attn.v_proj, lora_rank, lora_alpha)\n",
    "    if lora_o:\n",
    "        layer.self_attn.o_proj = LinearLoRALayer(layer.self_attn.o_proj, lora_rank, lora_alpha)\n",
    "    if lora_mlp:\n",
    "        # 替换MLP中的线性层\n",
    "        layer.mlp.gate_proj = LinearLoRALayer(layer.mlp.gate_proj, lora_rank, lora_alpha)\n",
    "        layer.mlp.up_proj = LinearLoRALayer(layer.mlp.up_proj, lora_rank, lora_alpha)\n",
    "        layer.mlp.down_proj = LinearLoRALayer(layer.mlp.down_proj, lora_rank, lora_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20fc9ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): LinearLoRALayer(\n",
       "            (linear): Linear(in_features=896, out_features=896, bias=True)\n",
       "            (lora_layer): LoRALayer()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): LinearLoRALayer(\n",
       "            (linear): Linear(in_features=896, out_features=128, bias=True)\n",
       "            (lora_layer): LoRALayer()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (o_proj): LinearLoRALayer(\n",
       "            (linear): Linear(in_features=896, out_features=896, bias=False)\n",
       "            (lora_layer): LoRALayer()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): LinearLoRALayer(\n",
       "            (linear): Linear(in_features=896, out_features=4864, bias=False)\n",
       "            (lora_layer): LoRALayer()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (up_proj): LinearLoRALayer(\n",
       "            (linear): Linear(in_features=896, out_features=4864, bias=False)\n",
       "            (lora_layer): LoRALayer()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (down_proj): LinearLoRALayer(\n",
       "            (linear): Linear(in_features=4864, out_features=896, bias=False)\n",
       "            (lora_layer): LoRALayer()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1202b3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if 'lora' not in name:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af1d630c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn.q_proj.lora_layer.lora_a\n",
      "model.layers.0.self_attn.q_proj.lora_layer.lora_b\n",
      "model.layers.0.self_attn.v_proj.lora_layer.lora_a\n",
      "model.layers.0.self_attn.v_proj.lora_layer.lora_b\n",
      "model.layers.0.self_attn.o_proj.lora_layer.lora_a\n",
      "model.layers.0.self_attn.o_proj.lora_layer.lora_b\n",
      "model.layers.0.mlp.gate_proj.lora_layer.lora_a\n",
      "model.layers.0.mlp.gate_proj.lora_layer.lora_b\n",
      "model.layers.0.mlp.up_proj.lora_layer.lora_a\n",
      "model.layers.0.mlp.up_proj.lora_layer.lora_b\n",
      "model.layers.0.mlp.down_proj.lora_layer.lora_a\n",
      "model.layers.0.mlp.down_proj.lora_layer.lora_b\n",
      "model.layers.1.self_attn.q_proj.lora_layer.lora_a\n",
      "model.layers.1.self_attn.q_proj.lora_layer.lora_b\n",
      "model.layers.1.self_attn.v_proj.lora_layer.lora_a\n",
      "model.layers.1.self_attn.v_proj.lora_layer.lora_b\n",
      "model.layers.1.self_attn.o_proj.lora_layer.lora_a\n",
      "model.layers.1.self_attn.o_proj.lora_layer.lora_b\n",
      "model.layers.1.mlp.gate_proj.lora_layer.lora_a\n",
      "model.layers.1.mlp.gate_proj.lora_layer.lora_b\n",
      "model.layers.1.mlp.up_proj.lora_layer.lora_a\n",
      "model.layers.1.mlp.up_proj.lora_layer.lora_b\n",
      "model.layers.1.mlp.down_proj.lora_layer.lora_a\n",
      "model.layers.1.mlp.down_proj.lora_layer.lora_b\n",
      "model.layers.2.self_attn.q_proj.lora_layer.lora_a\n",
      "model.layers.2.self_attn.q_proj.lora_layer.lora_b\n",
      "model.layers.2.self_attn.v_proj.lora_layer.lora_a\n",
      "model.layers.2.self_attn.v_proj.lora_layer.lora_b\n",
      "model.layers.2.self_attn.o_proj.lora_layer.lora_a\n",
      "model.layers.2.self_attn.o_proj.lora_layer.lora_b\n",
      "model.layers.2.mlp.gate_proj.lora_layer.lora_a\n",
      "model.layers.2.mlp.gate_proj.lora_layer.lora_b\n",
      "model.layers.2.mlp.up_proj.lora_layer.lora_a\n",
      "model.layers.2.mlp.up_proj.lora_layer.lora_b\n",
      "model.layers.2.mlp.down_proj.lora_layer.lora_a\n",
      "model.layers.2.mlp.down_proj.lora_layer.lora_b\n",
      "model.layers.3.self_attn.q_proj.lora_layer.lora_a\n",
      "model.layers.3.self_attn.q_proj.lora_layer.lora_b\n",
      "model.layers.3.self_attn.v_proj.lora_layer.lora_a\n",
      "model.layers.3.self_attn.v_proj.lora_layer.lora_b\n",
      "model.layers.3.self_attn.o_proj.lora_layer.lora_a\n",
      "model.layers.3.self_attn.o_proj.lora_layer.lora_b\n",
      "model.layers.3.mlp.gate_proj.lora_layer.lora_a\n",
      "model.layers.3.mlp.gate_proj.lora_layer.lora_b\n",
      "model.layers.3.mlp.up_proj.lora_layer.lora_a\n",
      "model.layers.3.mlp.up_proj.lora_layer.lora_b\n",
      "model.layers.3.mlp.down_proj.lora_layer.lora_a\n",
      "model.layers.3.mlp.down_proj.lora_layer.lora_b\n",
      "model.layers.4.self_attn.q_proj.lora_layer.lora_a\n",
      "model.layers.4.self_attn.q_proj.lora_layer.lora_b\n",
      "model.layers.4.self_attn.v_proj.lora_layer.lora_a\n",
      "model.layers.4.self_attn.v_proj.lora_layer.lora_b\n",
      "model.layers.4.self_attn.o_proj.lora_layer.lora_a\n",
      "model.layers.4.self_attn.o_proj.lora_layer.lora_b\n",
      "model.layers.4.mlp.gate_proj.lora_layer.lora_a\n",
      "model.layers.4.mlp.gate_proj.lora_layer.lora_b\n",
      "model.layers.4.mlp.up_proj.lora_layer.lora_a\n",
      "model.layers.4.mlp.up_proj.lora_layer.lora_b\n",
      "model.layers.4.mlp.down_proj.lora_layer.lora_a\n",
      "model.layers.4.mlp.down_proj.lora_layer.lora_b\n",
      "model.layers.5.self_attn.q_proj.lora_layer.lora_a\n",
      "model.layers.5.self_attn.q_proj.lora_layer.lora_b\n",
      "model.layers.5.self_attn.v_proj.lora_layer.lora_a\n",
      "model.layers.5.self_attn.v_proj.lora_layer.lora_b\n",
      "model.layers.5.self_attn.o_proj.lora_layer.lora_a\n",
      "model.layers.5.self_attn.o_proj.lora_layer.lora_b\n",
      "model.layers.5.mlp.gate_proj.lora_layer.lora_a\n",
      "model.layers.5.mlp.gate_proj.lora_layer.lora_b\n",
      "model.layers.5.mlp.up_proj.lora_layer.lora_a\n",
      "model.layers.5.mlp.up_proj.lora_layer.lora_b\n",
      "model.layers.5.mlp.down_proj.lora_layer.lora_a\n",
      "model.layers.5.mlp.down_proj.lora_layer.lora_b\n",
      "model.layers.6.self_attn.q_proj.lora_layer.lora_a\n",
      "model.layers.6.self_attn.q_proj.lora_layer.lora_b\n",
      "model.layers.6.self_attn.v_proj.lora_layer.lora_a\n",
      "model.layers.6.self_attn.v_proj.lora_layer.lora_b\n",
      "model.layers.6.self_attn.o_proj.lora_layer.lora_a\n",
      "model.layers.6.self_attn.o_proj.lora_layer.lora_b\n",
      "model.layers.6.mlp.gate_proj.lora_layer.lora_a\n",
      "model.layers.6.mlp.gate_proj.lora_layer.lora_b\n",
      "model.layers.6.mlp.up_proj.lora_layer.lora_a\n",
      "model.layers.6.mlp.up_proj.lora_layer.lora_b\n",
      "model.layers.6.mlp.down_proj.lora_layer.lora_a\n",
      "model.layers.6.mlp.down_proj.lora_layer.lora_b\n",
      "model.layers.7.self_attn.q_proj.lora_layer.lora_a\n",
      "model.layers.7.self_attn.q_proj.lora_layer.lora_b\n",
      "model.layers.7.self_attn.v_proj.lora_layer.lora_a\n",
      "model.layers.7.self_attn.v_proj.lora_layer.lora_b\n",
      "model.layers.7.self_attn.o_proj.lora_layer.lora_a\n",
      "model.layers.7.self_attn.o_proj.lora_layer.lora_b\n",
      "model.layers.7.mlp.gate_proj.lora_layer.lora_a\n",
      "model.layers.7.mlp.gate_proj.lora_layer.lora_b\n",
      "model.layers.7.mlp.up_proj.lora_layer.lora_a\n",
      "model.layers.7.mlp.up_proj.lora_layer.lora_b\n",
      "model.layers.7.mlp.down_proj.lora_layer.lora_a\n",
      "model.layers.7.mlp.down_proj.lora_layer.lora_b\n",
      "model.layers.8.self_attn.q_proj.lora_layer.lora_a\n",
      "model.layers.8.self_attn.q_proj.lora_layer.lora_b\n",
      "model.layers.8.self_attn.v_proj.lora_layer.lora_a\n",
      "model.layers.8.self_attn.v_proj.lora_layer.lora_b\n",
      "model.layers.8.self_attn.o_proj.lora_layer.lora_a\n",
      "model.layers.8.self_attn.o_proj.lora_layer.lora_b\n",
      "model.layers.8.mlp.gate_proj.lora_layer.lora_a\n",
      "model.layers.8.mlp.gate_proj.lora_layer.lora_b\n",
      "model.layers.8.mlp.up_proj.lora_layer.lora_a\n",
      "model.layers.8.mlp.up_proj.lora_layer.lora_b\n",
      "model.layers.8.mlp.down_proj.lora_layer.lora_a\n",
      "model.layers.8.mlp.down_proj.lora_layer.lora_b\n",
      "model.layers.9.self_attn.q_proj.lora_layer.lora_a\n",
      "model.layers.9.self_attn.q_proj.lora_layer.lora_b\n",
      "model.layers.9.self_attn.v_proj.lora_layer.lora_a\n",
      "model.layers.9.self_attn.v_proj.lora_layer.lora_b\n",
      "model.layers.9.self_attn.o_proj.lora_layer.lora_a\n",
      "model.layers.9.self_attn.o_proj.lora_layer.lora_b\n",
      "model.layers.9.mlp.gate_proj.lora_layer.lora_a\n",
      "model.layers.9.mlp.gate_proj.lora_layer.lora_b\n",
      "model.layers.9.mlp.up_proj.lora_layer.lora_a\n",
      "model.layers.9.mlp.up_proj.lora_layer.lora_b\n",
      "model.layers.9.mlp.down_proj.lora_layer.lora_a\n",
      "model.layers.9.mlp.down_proj.lora_layer.lora_b\n",
      "model.layers.10.self_attn.q_proj.lora_layer.lora_a\n",
      "model.layers.10.self_attn.q_proj.lora_layer.lora_b\n",
      "model.layers.10.self_attn.v_proj.lora_layer.lora_a\n",
      "model.layers.10.self_attn.v_proj.lora_layer.lora_b\n",
      "model.layers.10.self_attn.o_proj.lora_layer.lora_a\n",
      "model.layers.10.self_attn.o_proj.lora_layer.lora_b\n",
      "model.layers.10.mlp.gate_proj.lora_layer.lora_a\n",
      "model.layers.10.mlp.gate_proj.lora_layer.lora_b\n",
      "model.layers.10.mlp.up_proj.lora_layer.lora_a\n",
      "model.layers.10.mlp.up_proj.lora_layer.lora_b\n",
      "model.layers.10.mlp.down_proj.lora_layer.lora_a\n",
      "model.layers.10.mlp.down_proj.lora_layer.lora_b\n",
      "model.layers.11.self_attn.q_proj.lora_layer.lora_a\n",
      "model.layers.11.self_attn.q_proj.lora_layer.lora_b\n",
      "model.layers.11.self_attn.v_proj.lora_layer.lora_a\n",
      "model.layers.11.self_attn.v_proj.lora_layer.lora_b\n",
      "model.layers.11.self_attn.o_proj.lora_layer.lora_a\n",
      "model.layers.11.self_attn.o_proj.lora_layer.lora_b\n",
      "model.layers.11.mlp.gate_proj.lora_layer.lora_a\n",
      "model.layers.11.mlp.gate_proj.lora_layer.lora_b\n",
      "model.layers.11.mlp.up_proj.lora_layer.lora_a\n",
      "model.layers.11.mlp.up_proj.lora_layer.lora_b\n",
      "model.layers.11.mlp.down_proj.lora_layer.lora_a\n",
      "model.layers.11.mlp.down_proj.lora_layer.lora_b\n",
      "model.layers.12.self_attn.q_proj.lora_layer.lora_a\n",
      "model.layers.12.self_attn.q_proj.lora_layer.lora_b\n",
      "model.layers.12.self_attn.v_proj.lora_layer.lora_a\n",
      "model.layers.12.self_attn.v_proj.lora_layer.lora_b\n",
      "model.layers.12.self_attn.o_proj.lora_layer.lora_a\n",
      "model.layers.12.self_attn.o_proj.lora_layer.lora_b\n",
      "model.layers.12.mlp.gate_proj.lora_layer.lora_a\n",
      "model.layers.12.mlp.gate_proj.lora_layer.lora_b\n",
      "model.layers.12.mlp.up_proj.lora_layer.lora_a\n",
      "model.layers.12.mlp.up_proj.lora_layer.lora_b\n",
      "model.layers.12.mlp.down_proj.lora_layer.lora_a\n",
      "model.layers.12.mlp.down_proj.lora_layer.lora_b\n",
      "model.layers.13.self_attn.q_proj.lora_layer.lora_a\n",
      "model.layers.13.self_attn.q_proj.lora_layer.lora_b\n",
      "model.layers.13.self_attn.v_proj.lora_layer.lora_a\n",
      "model.layers.13.self_attn.v_proj.lora_layer.lora_b\n",
      "model.layers.13.self_attn.o_proj.lora_layer.lora_a\n",
      "model.layers.13.self_attn.o_proj.lora_layer.lora_b\n",
      "model.layers.13.mlp.gate_proj.lora_layer.lora_a\n",
      "model.layers.13.mlp.gate_proj.lora_layer.lora_b\n",
      "model.layers.13.mlp.up_proj.lora_layer.lora_a\n",
      "model.layers.13.mlp.up_proj.lora_layer.lora_b\n",
      "model.layers.13.mlp.down_proj.lora_layer.lora_a\n",
      "model.layers.13.mlp.down_proj.lora_layer.lora_b\n",
      "model.layers.14.self_attn.q_proj.lora_layer.lora_a\n",
      "model.layers.14.self_attn.q_proj.lora_layer.lora_b\n",
      "model.layers.14.self_attn.v_proj.lora_layer.lora_a\n",
      "model.layers.14.self_attn.v_proj.lora_layer.lora_b\n",
      "model.layers.14.self_attn.o_proj.lora_layer.lora_a\n",
      "model.layers.14.self_attn.o_proj.lora_layer.lora_b\n",
      "model.layers.14.mlp.gate_proj.lora_layer.lora_a\n",
      "model.layers.14.mlp.gate_proj.lora_layer.lora_b\n",
      "model.layers.14.mlp.up_proj.lora_layer.lora_a\n",
      "model.layers.14.mlp.up_proj.lora_layer.lora_b\n",
      "model.layers.14.mlp.down_proj.lora_layer.lora_a\n",
      "model.layers.14.mlp.down_proj.lora_layer.lora_b\n",
      "model.layers.15.self_attn.q_proj.lora_layer.lora_a\n",
      "model.layers.15.self_attn.q_proj.lora_layer.lora_b\n",
      "model.layers.15.self_attn.v_proj.lora_layer.lora_a\n",
      "model.layers.15.self_attn.v_proj.lora_layer.lora_b\n",
      "model.layers.15.self_attn.o_proj.lora_layer.lora_a\n",
      "model.layers.15.self_attn.o_proj.lora_layer.lora_b\n",
      "model.layers.15.mlp.gate_proj.lora_layer.lora_a\n",
      "model.layers.15.mlp.gate_proj.lora_layer.lora_b\n",
      "model.layers.15.mlp.up_proj.lora_layer.lora_a\n",
      "model.layers.15.mlp.up_proj.lora_layer.lora_b\n",
      "model.layers.15.mlp.down_proj.lora_layer.lora_a\n",
      "model.layers.15.mlp.down_proj.lora_layer.lora_b\n",
      "model.layers.16.self_attn.q_proj.lora_layer.lora_a\n",
      "model.layers.16.self_attn.q_proj.lora_layer.lora_b\n",
      "model.layers.16.self_attn.v_proj.lora_layer.lora_a\n",
      "model.layers.16.self_attn.v_proj.lora_layer.lora_b\n",
      "model.layers.16.self_attn.o_proj.lora_layer.lora_a\n",
      "model.layers.16.self_attn.o_proj.lora_layer.lora_b\n",
      "model.layers.16.mlp.gate_proj.lora_layer.lora_a\n",
      "model.layers.16.mlp.gate_proj.lora_layer.lora_b\n",
      "model.layers.16.mlp.up_proj.lora_layer.lora_a\n",
      "model.layers.16.mlp.up_proj.lora_layer.lora_b\n",
      "model.layers.16.mlp.down_proj.lora_layer.lora_a\n",
      "model.layers.16.mlp.down_proj.lora_layer.lora_b\n",
      "model.layers.17.self_attn.q_proj.lora_layer.lora_a\n",
      "model.layers.17.self_attn.q_proj.lora_layer.lora_b\n",
      "model.layers.17.self_attn.v_proj.lora_layer.lora_a\n",
      "model.layers.17.self_attn.v_proj.lora_layer.lora_b\n",
      "model.layers.17.self_attn.o_proj.lora_layer.lora_a\n",
      "model.layers.17.self_attn.o_proj.lora_layer.lora_b\n",
      "model.layers.17.mlp.gate_proj.lora_layer.lora_a\n",
      "model.layers.17.mlp.gate_proj.lora_layer.lora_b\n",
      "model.layers.17.mlp.up_proj.lora_layer.lora_a\n",
      "model.layers.17.mlp.up_proj.lora_layer.lora_b\n",
      "model.layers.17.mlp.down_proj.lora_layer.lora_a\n",
      "model.layers.17.mlp.down_proj.lora_layer.lora_b\n",
      "model.layers.18.self_attn.q_proj.lora_layer.lora_a\n",
      "model.layers.18.self_attn.q_proj.lora_layer.lora_b\n",
      "model.layers.18.self_attn.v_proj.lora_layer.lora_a\n",
      "model.layers.18.self_attn.v_proj.lora_layer.lora_b\n",
      "model.layers.18.self_attn.o_proj.lora_layer.lora_a\n",
      "model.layers.18.self_attn.o_proj.lora_layer.lora_b\n",
      "model.layers.18.mlp.gate_proj.lora_layer.lora_a\n",
      "model.layers.18.mlp.gate_proj.lora_layer.lora_b\n",
      "model.layers.18.mlp.up_proj.lora_layer.lora_a\n",
      "model.layers.18.mlp.up_proj.lora_layer.lora_b\n",
      "model.layers.18.mlp.down_proj.lora_layer.lora_a\n",
      "model.layers.18.mlp.down_proj.lora_layer.lora_b\n",
      "model.layers.19.self_attn.q_proj.lora_layer.lora_a\n",
      "model.layers.19.self_attn.q_proj.lora_layer.lora_b\n",
      "model.layers.19.self_attn.v_proj.lora_layer.lora_a\n",
      "model.layers.19.self_attn.v_proj.lora_layer.lora_b\n",
      "model.layers.19.self_attn.o_proj.lora_layer.lora_a\n",
      "model.layers.19.self_attn.o_proj.lora_layer.lora_b\n",
      "model.layers.19.mlp.gate_proj.lora_layer.lora_a\n",
      "model.layers.19.mlp.gate_proj.lora_layer.lora_b\n",
      "model.layers.19.mlp.up_proj.lora_layer.lora_a\n",
      "model.layers.19.mlp.up_proj.lora_layer.lora_b\n",
      "model.layers.19.mlp.down_proj.lora_layer.lora_a\n",
      "model.layers.19.mlp.down_proj.lora_layer.lora_b\n",
      "model.layers.20.self_attn.q_proj.lora_layer.lora_a\n",
      "model.layers.20.self_attn.q_proj.lora_layer.lora_b\n",
      "model.layers.20.self_attn.v_proj.lora_layer.lora_a\n",
      "model.layers.20.self_attn.v_proj.lora_layer.lora_b\n",
      "model.layers.20.self_attn.o_proj.lora_layer.lora_a\n",
      "model.layers.20.self_attn.o_proj.lora_layer.lora_b\n",
      "model.layers.20.mlp.gate_proj.lora_layer.lora_a\n",
      "model.layers.20.mlp.gate_proj.lora_layer.lora_b\n",
      "model.layers.20.mlp.up_proj.lora_layer.lora_a\n",
      "model.layers.20.mlp.up_proj.lora_layer.lora_b\n",
      "model.layers.20.mlp.down_proj.lora_layer.lora_a\n",
      "model.layers.20.mlp.down_proj.lora_layer.lora_b\n",
      "model.layers.21.self_attn.q_proj.lora_layer.lora_a\n",
      "model.layers.21.self_attn.q_proj.lora_layer.lora_b\n",
      "model.layers.21.self_attn.v_proj.lora_layer.lora_a\n",
      "model.layers.21.self_attn.v_proj.lora_layer.lora_b\n",
      "model.layers.21.self_attn.o_proj.lora_layer.lora_a\n",
      "model.layers.21.self_attn.o_proj.lora_layer.lora_b\n",
      "model.layers.21.mlp.gate_proj.lora_layer.lora_a\n",
      "model.layers.21.mlp.gate_proj.lora_layer.lora_b\n",
      "model.layers.21.mlp.up_proj.lora_layer.lora_a\n",
      "model.layers.21.mlp.up_proj.lora_layer.lora_b\n",
      "model.layers.21.mlp.down_proj.lora_layer.lora_a\n",
      "model.layers.21.mlp.down_proj.lora_layer.lora_b\n",
      "model.layers.22.self_attn.q_proj.lora_layer.lora_a\n",
      "model.layers.22.self_attn.q_proj.lora_layer.lora_b\n",
      "model.layers.22.self_attn.v_proj.lora_layer.lora_a\n",
      "model.layers.22.self_attn.v_proj.lora_layer.lora_b\n",
      "model.layers.22.self_attn.o_proj.lora_layer.lora_a\n",
      "model.layers.22.self_attn.o_proj.lora_layer.lora_b\n",
      "model.layers.22.mlp.gate_proj.lora_layer.lora_a\n",
      "model.layers.22.mlp.gate_proj.lora_layer.lora_b\n",
      "model.layers.22.mlp.up_proj.lora_layer.lora_a\n",
      "model.layers.22.mlp.up_proj.lora_layer.lora_b\n",
      "model.layers.22.mlp.down_proj.lora_layer.lora_a\n",
      "model.layers.22.mlp.down_proj.lora_layer.lora_b\n",
      "model.layers.23.self_attn.q_proj.lora_layer.lora_a\n",
      "model.layers.23.self_attn.q_proj.lora_layer.lora_b\n",
      "model.layers.23.self_attn.v_proj.lora_layer.lora_a\n",
      "model.layers.23.self_attn.v_proj.lora_layer.lora_b\n",
      "model.layers.23.self_attn.o_proj.lora_layer.lora_a\n",
      "model.layers.23.self_attn.o_proj.lora_layer.lora_b\n",
      "model.layers.23.mlp.gate_proj.lora_layer.lora_a\n",
      "model.layers.23.mlp.gate_proj.lora_layer.lora_b\n",
      "model.layers.23.mlp.up_proj.lora_layer.lora_a\n",
      "model.layers.23.mlp.up_proj.lora_layer.lora_b\n",
      "model.layers.23.mlp.down_proj.lora_layer.lora_a\n",
      "model.layers.23.mlp.down_proj.lora_layer.lora_b\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        print(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseek2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
