{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "428f521d",
   "metadata": {},
   "source": [
    "# 激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a80242",
   "metadata": {},
   "source": [
    "如果没有激活函数，只有线性变换，无论神经网络有多少层，输出都是输入的线性组合，最终会坍缩成一个线性函数，无法处理非线性问题。激活函数的作用就是引入非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中。  \n",
    "激活函数特点：1、非线性；2、可微性；3、单调性；4、定义域为实数区间  \n",
    "\n",
    "饱和函数是指在输入值趋于正无穷和负无穷时，其导数趋近于零的函数。\n",
    "\n",
    "视频1：https://www.bilibili.com/video/BV1qB4y1e7GJ/?spm_id_from=333.337.search-card.all.click&vd_source=071b23b9c7175dbaf674c65294124341"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884435ae",
   "metadata": {},
   "source": [
    "## 1、sigmoid函数\n",
    "sigmoid函数将实数压缩到0-1之间，公式为：\n",
    "$$\n",
    "f(y)=\\frac{1}{1+e^{-y}}\n",
    "$$\n",
    "sigmoid函数的图像如下，为非零均值函数：\n",
    "  \n",
    "<div align=\"center\">\n",
    "    <img src=\"激活函数/sigmoid.png\" alt=\"图片描述\" width=\"200\"/>\n",
    "</div>\n",
    "\n",
    "sigmoid函数的导数公式为：\n",
    "$$\n",
    "f'(y)=f(y)(1-f(y))\n",
    "$$\n",
    "sigmoid函数的导数图像如下，为饱和函数：\n",
    "  \n",
    "<div align=\"center\">\n",
    "    <img src=\"激活函数/sigmoid导数.png\" alt=\"图片描述\" width=\"200\"/>\n",
    "</div>\n",
    "\n",
    "sigmoid函数存在的问题：  \n",
    "1、sigmoid函数的导数图像在y=0处有一个极值，为0.25比较小，输出值会不停被缩小，并且在y非常大和非常小时，梯度趋近于0，因此，在反向传播时，梯度更新会非常小，甚至导致梯度消失问题。\n",
    "\n",
    "2、sigmoid的输出始终大于0，这意味着下一层神经元的输入（即前一层的输出）也始终为正值，导致在反向传播时，权重更新时，权重更新方向始终一致，导致权重更新不稳定，不易收敛。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fce0df",
   "metadata": {},
   "source": [
    "## 2、tanh函数\n",
    "tanh函数将实数压缩到-1-1之间，公式为：\n",
    "$$\n",
    "f(y)=\\frac{1-e^{-y}}{1+e^{-y}}\n",
    "$$\n",
    "tanh函数的图像如下，为非零均值函数：\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"激活函数/tanh.png\" alt=\"图片描述\" width=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5effb45b",
   "metadata": {},
   "source": [
    "## 3、ReLU函数\n",
    "ReLU函数将小于0的值置为0，大于0的值保持不变，公式为：\n",
    "$$\n",
    "f(y)=max(0,y)\n",
    "$$\n",
    "ReLU函数的图像如下，为非零均值函数：\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"激活函数/ReLU.png\" alt=\"图片描述\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "ReLU函数存在的问题：\n",
    "1、ReLU函数梯度爆炸问题，当输入值大于0时，梯度为1，当输入值小于0时，梯度为0，当输入值非常大时，梯度更新会非常大，导致权重更新不稳定，不易收敛。  \n",
    "\n",
    "2、ReLU函数在y<0时，梯度为0，导致在反向传播时，权重更新会停止，导致神经元“死亡”，即不再对任何数据有响应。\n",
    "\n",
    "### 改进\n",
    "1、Leaky ReLU函数，当y<0时，梯度为0.01，公式为：\n",
    "$$\n",
    "f(y)=max(0.01y,y)\n",
    "$$\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"激活函数/L-ReLU.png\" alt=\"图片描述\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "2、Parametric ReLU函数，当y<0时，梯度为α，α为可学习的参数，公式为：\n",
    "$$\n",
    "f(y)=max(αy,y)\n",
    "$$\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"激活函数/P-ReLU.png\" alt=\"图片描述\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "3、Exponential Linear Unit函数，公式为：\n",
    "$$\n",
    "f(y)=\\begin{cases}y&y>0\\\\ \\alpha(e^y-1)&y\\leq 0\\end{cases}\n",
    "$$\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"激活函数/ELU.png\" alt=\"图片描述\" width=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a148455",
   "metadata": {},
   "source": [
    "## 4、swish函数(SiLU)\n",
    "swish函数公式为：\n",
    "$$\n",
    "f(y)=y*sigmoid(\\beta y)\n",
    "$$\n",
    "swish函数的图像如下，为非零均值函数：\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"激活函数/Swish.png\" alt=\"图片描述\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "swish函数的导数公式为：\n",
    "$$\n",
    "f'(y)=\\beta f(y)+sigmoid(\\beta y)(1-\\beta f(y))\n",
    "$$\n",
    "swish函数的导数图像如下：\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"激活函数/Swish导数.png\" alt=\"图片描述\" width=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc401f3",
   "metadata": {},
   "source": [
    "## 5、GELU函数\n",
    "GELU函数公式为：\n",
    "\n",
    "$$\n",
    "f(y)=y \\cdot \\Phi(y)\n",
    "$$\n",
    "其中，Φ(y)是标准正态分布的累积分布函数，近似公式为：  \n",
    "$$\n",
    "\\Phi(y)=0.5\\left[1+\\text{erf}\\left(\\frac{y}{\\sqrt{2}}\\right)\\right]\n",
    "$$\n",
    "其中，erf是误差函数，近似公式为：  \n",
    "$$\n",
    "\\text{erf}(y)=\\frac{2}{\\sqrt{\\pi}}\\int_{0}^{y}e^{-t^2}dt\n",
    "$$\n",
    "近似表达式为：  \n",
    "$$\n",
    "f(y)=0.5y(1+tanh(\\sqrt{\\frac{2}{\\pi}}(y+0.044715y^3)))\n",
    "$$\n",
    "GELU函数的图像如下，为非零均值函数：\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"激活函数/GELU.png\" alt=\"图片描述\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "GELU函数的导数公式为：\n",
    "$$\n",
    "f'(y)=0.5(1+tanh(\\sqrt{\\frac{2}{\\pi}}(y+0.044715y^3)))+0.5y\\frac{1}{\\sqrt{\\frac{2}{\\pi}}(y+0.044715y^3)}(1-tanh^2(\\sqrt{\\frac{2}{\\pi}}(y+0.044715y^3)))\n",
    "$$\n",
    "GELU函数的导数图像如下：\n",
    "\n",
    "<div align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0634f9dd",
   "metadata": {},
   "source": [
    "## 6、SwiGLU函数\n",
    "### GLU\n",
    "Gated Linear Unit函数，公式为：\n",
    "$$\n",
    "GLU(x, W, V, b, c) = \\sigma(Wx+b) \\otimes (Vx+c)\n",
    "$$\n",
    "其中$\\sigma$在原论文中为sigmoid函数，x为输入，W为权重矩阵，b为偏置，V为权重矩阵，c为偏置，$\\otimes$为逐元素乘法。\n",
    "\n",
    "### SwiGLU\n",
    "SwiGLU函数，公式为：\n",
    "$$\n",
    "SwiGLU(x, W, V, b, c) = \\sigma(Wx+b) \\otimes (Vx+c)\n",
    "$$\n",
    "其中$\\sigma$变为了swish函数\n",
    "\n",
    "LLaMA中结构如下图所示：\n",
    "<div align=\"center\">\n",
    "    <img src=\"激活函数/llama中FFN.png\" alt=\"图片描述\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26fd4f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "# -*- coding  : utf-8 -*-\n",
    "# Author: honggao.zhang\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FFNSwiGLU(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim, bias=False)\n",
    "        self.fc2 = nn.Linear(hidden_dim, input_dim, bias=False)\n",
    "        self.fc3 = nn.Linear(input_dim, hidden_dim, bias=False) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # LLaMA 官方提供的代码是使用 F.silu() 激活函数\n",
    "        return self.fc2(F.silu(self.fc1(x)) * self.fc3(x))\n",
    "    \n",
    "layer = FFNSwiGLU(128, 256)\n",
    "x = torch.randn(1, 128)\n",
    "out = layer(x)\n",
    "print(out.shape) # torch.Size([1, 128])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bcf499",
   "metadata": {},
   "source": [
    "## 部分激活函数对比\n",
    "<div align=\"center\">\n",
    "    <img src=\"激活函数/各激活函数对比.png\" alt=\"图片描述\" width=\"500\"/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseek2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
