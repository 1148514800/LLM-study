{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "875400a6",
   "metadata": {},
   "source": [
    "# deepseed和accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258ad09d",
   "metadata": {},
   "source": [
    "视频一：https://www.bilibili.com/video/BV1ZZ421T7XJ/?spm_id_from=333.337.search-card.all.click&vd_source=071b23b9c7175dbaf674c65294124341  \n",
    "视频二：https://www.bilibili.com/video/BV1uK421a7HG?spm_id_from=333.788.videopod.episodes&vd_source=071b23b9c7175dbaf674c65294124341&p=4\n",
    "官方博客：https://huggingface.co/docs/accelerate/quicktour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2392d194",
   "metadata": {},
   "source": [
    "### 区别  \n",
    "DeepSpeed 是由 Microsoft 开发的一个深度学习优化库，旨在提高大规模模型训练的效率。它提供了多种优化技术，包括混合精度训练、分布式训练、数据并行、模型并行和高效的梯度累积等。  \n",
    "主要特点：  \n",
    "1. 分布式训练: 支持数据并行、模型并行（包括管道并行和张量并行），使得训练超大规模模型成为可能。  \n",
    "2. 优化技术: 提供优化算法，如 ZeRO（Zero Redundancy Optimizer）以减少显存占用、加速训练速度。  \n",
    "3. 混合精度训练: 自动支持 FP16 和 BFLOAT16 精度训练，以减少计算开销和内存占用。  \n",
    "4. 高效的梯度累积: 提供高效的梯度累积方法来处理超大批量训练。  \n",
    "5. 弹性训练: 支持弹性训练，允许动态添加或移除计算资源。  \n",
    "功能非常的强大、丰富，但是配置起来比较复杂，需要一定的深度学习知识。  \n",
    "\n",
    "Accelerate 是由 Hugging Face 开发的一个库，旨在简化分布式训练的设置。它提供了一种简洁的方式来配置和管理多 GPU 和 TPU 环境，支持数据并行和模型并行。  \n",
    "主要特点  \n",
    "1. 简化配置: 提供统一的接口来处理多 GPU 和 TPU 环境的配置，简化了分布式训练的复杂性。  \n",
    "2. 支持多种后端: 可以与不同的深度学习后端（如 PyTorch 和 TensorFlow）集成。  \n",
    "3. 集成 DeepSpeed: 可以与 DeepSpeed 集成，利用 DeepSpeed 的高级功能进行训练。  \n",
    "4. 简化分布式训练: 自动处理分布式训练的设置和同步问题，使用户能够专注于模型和数据  \n",
    "\n",
    "功能丰富，而且使用非常简单，但是配置不是非常精细。  \n",
    "联系  \n",
    "1. 集成能力: Accelerate 可以与 DeepSpeed 集成，利用 DeepSpeed 的高级功能来优化训练。通过 Accelerate 的 DeepSpeedPlugin，可以在 Accelerate 的框架下使用 DeepSpeed 进行训练。  \n",
    "2. 共同目标: 两者都旨在提高大规模模型训练的效率和简化配置，但 DeepSpeed 提供了更多的优化功能，而 Accelerate 注重于简化配置和多后端支持。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2bffad",
   "metadata": {},
   "source": [
    "## accelerate为什么能加速？  \n",
    "\n",
    "1、数据并行：在数据并行中，模型的不同部分在不同的 GPU 上运行，每个 GPU 处理数据的不同部分。这样，可以并行处理数据，从而加速训练过程。  \n",
    "2、梯度同步：在每个反向传播步骤中，Accelerate 会同步所有 GPU 上的梯度。具体步骤如下：1、每个 GPU 计算自己的梯度。2、所有 GPU 上的梯度通过 all_reduce 操作进行同步，确保每个 GPU 上的梯度是全局一致的。3、每个 GPU 使用同步后的梯度更新模型参数。  \n",
    "3、模型并行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9507f3ba",
   "metadata": {},
   "source": [
    "## accelerate基础函数\n",
    "\n",
    "1、 accelerator.reduce()：在 Accelerate 中，accelerator.reduce() 是一个非常有用的函数，用于在分布式训练环境中对多个设备上的张量进行聚合操作（如求和、平均等）。  \n",
    "accelerator.reduce(loss, reduction=\"mean\")\n",
    "\n",
    "2、accelerator.gather()：在 Accelerate 中，accelerator.gather() 是一个用于在分布式训练环境中收集多个设备上的张量到一个设备上的函数。  \n",
    "accelerator.gather(loss).mean()即等价于accelerator.reduce(loss, reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26db0952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on one GPU.\n",
      "Epoch 0 loss: 1.1368621587753296\n",
      "Epoch 1 loss: 1.6605136394500732\n",
      "Epoch 2 loss: 1.13680100440979\n",
      "Epoch 3 loss: 1.0083215236663818\n",
      "Epoch 4 loss: 0.5586246252059937\n",
      "Epoch 5 loss: 0.5666735172271729\n",
      "Epoch 6 loss: 0.5323970317840576\n",
      "Epoch 7 loss: 0.7227011919021606\n",
      "Epoch 8 loss: 0.9263297319412231\n",
      "Epoch 9 loss: 0.9665447473526001\n",
      "Epoch 10 loss: 1.2193164825439453\n",
      "Epoch 11 loss: 0.8520088195800781\n",
      "Epoch 12 loss: 1.3043696880340576\n",
      "Epoch 13 loss: 1.1880245208740234\n",
      "Epoch 14 loss: 0.989682674407959\n",
      "Epoch 15 loss: 1.0800554752349854\n",
      "Epoch 16 loss: 1.2249712944030762\n",
      "Epoch 17 loss: 1.0276449918746948\n",
      "Epoch 18 loss: 1.5330866575241089\n",
      "Epoch 19 loss: 0.9961369037628174\n",
      "Epoch 20 loss: 1.3767263889312744\n",
      "Epoch 21 loss: 0.8045156002044678\n",
      "Epoch 22 loss: 0.7873939871788025\n",
      "Epoch 23 loss: 0.8252222537994385\n",
      "Epoch 24 loss: 1.0691642761230469\n",
      "Epoch 25 loss: 0.761885404586792\n",
      "Epoch 26 loss: 0.841905951499939\n",
      "Epoch 27 loss: 0.6713285446166992\n",
      "Epoch 28 loss: 1.4273896217346191\n",
      "Epoch 29 loss: 1.6077072620391846\n",
      "Epoch 30 loss: 1.013512372970581\n",
      "Epoch 31 loss: 1.0060348510742188\n",
      "Epoch 32 loss: 1.01304030418396\n",
      "Epoch 33 loss: 0.6987271308898926\n",
      "Epoch 34 loss: 1.3762235641479492\n",
      "Epoch 35 loss: 0.9300245046615601\n",
      "Epoch 36 loss: 0.7286868095397949\n",
      "Epoch 37 loss: 1.0818835496902466\n",
      "Epoch 38 loss: 1.3268327713012695\n",
      "Epoch 39 loss: 0.9101468324661255\n",
      "Epoch 40 loss: 1.4079827070236206\n",
      "Epoch 41 loss: 0.5161714553833008\n",
      "Epoch 42 loss: 0.7943459749221802\n",
      "Epoch 43 loss: 1.2831778526306152\n",
      "Epoch 44 loss: 0.9375430345535278\n",
      "Epoch 45 loss: 1.0120725631713867\n",
      "Epoch 46 loss: 0.6910718679428101\n",
      "Epoch 47 loss: 0.6704229116439819\n",
      "Epoch 48 loss: 0.6855909824371338\n",
      "Epoch 49 loss: 0.598656952381134\n",
      "Epoch 50 loss: 0.635104775428772\n",
      "Epoch 51 loss: 0.9331446886062622\n",
      "Epoch 52 loss: 0.8199288845062256\n",
      "Epoch 53 loss: 0.780022144317627\n",
      "Epoch 54 loss: 1.0411481857299805\n",
      "Epoch 55 loss: 0.9873160719871521\n",
      "Epoch 56 loss: 1.066403865814209\n",
      "Epoch 57 loss: 1.292781949043274\n",
      "Epoch 58 loss: 0.8196269273757935\n",
      "Epoch 59 loss: 1.0948188304901123\n",
      "Epoch 60 loss: 0.9561618566513062\n",
      "Epoch 61 loss: 0.8445926308631897\n",
      "Epoch 62 loss: 0.9484962224960327\n",
      "Epoch 63 loss: 1.0310267210006714\n",
      "Epoch 64 loss: 1.0014880895614624\n",
      "Epoch 65 loss: 0.9902445673942566\n",
      "Epoch 66 loss: 0.8450592160224915\n",
      "Epoch 67 loss: 0.8663830757141113\n",
      "Epoch 68 loss: 1.3693842887878418\n",
      "Epoch 69 loss: 0.7396247386932373\n",
      "Epoch 70 loss: 1.3668233156204224\n",
      "Epoch 71 loss: 0.6976581811904907\n",
      "Epoch 72 loss: 1.2240077257156372\n",
      "Epoch 73 loss: 0.7397314310073853\n",
      "Epoch 74 loss: 0.8655388355255127\n",
      "Epoch 75 loss: 0.8580310344696045\n",
      "Epoch 76 loss: 0.8161239624023438\n",
      "Epoch 77 loss: 1.2347612380981445\n",
      "Epoch 78 loss: 0.8791144490242004\n",
      "Epoch 79 loss: 0.8033548593521118\n",
      "Epoch 80 loss: 0.776456356048584\n",
      "Epoch 81 loss: 1.2849962711334229\n",
      "Epoch 82 loss: 1.1525664329528809\n",
      "Epoch 83 loss: 1.0552289485931396\n",
      "Epoch 84 loss: 1.2160677909851074\n",
      "Epoch 85 loss: 0.5892541408538818\n",
      "Epoch 86 loss: 0.927729070186615\n",
      "Epoch 87 loss: 0.9056459069252014\n",
      "Epoch 88 loss: 1.2359342575073242\n",
      "Epoch 89 loss: 0.9106587767601013\n",
      "Epoch 90 loss: 1.1511592864990234\n",
      "Epoch 91 loss: 1.366081953048706\n",
      "Epoch 92 loss: 1.3357603549957275\n",
      "Epoch 93 loss: 0.879988431930542\n",
      "Epoch 94 loss: 0.6483293771743774\n",
      "Epoch 95 loss: 1.1015422344207764\n",
      "Epoch 96 loss: 0.97690749168396\n",
      "Epoch 97 loss: 1.265831708908081\n",
      "Epoch 98 loss: 0.6816220283508301\n",
      "Epoch 99 loss: 1.0322380065917969\n",
      "Training time: 25.63 seconds\n"
     ]
    }
   ],
   "source": [
    "# 版本一\n",
    "\n",
    "'''\n",
    "单卡直接运行：35.76 seconds\n",
    "单卡accelerate：48.82 seconds\n",
    "双卡accelerate：29.51 seconds\n",
    "'''\n",
    "\n",
    "from accelerate import Accelerator, DeepSpeedPlugin, notebook_launcher\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import time\n",
    "\n",
    "class SimpleNet(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)  #.to(\"cuda:0\")\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, output_dim) #.to(\"cuda:1\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.to(\"cuda:0\")\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        # x.to(\"cuda:1\")\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "def main():\n",
    "    input_dim = 10\n",
    "    hidden_dim = 20\n",
    "    output_dim = 2\n",
    "    batch_size = 64\n",
    "    data_size = 10000\n",
    "\n",
    "    device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    input_data = torch.randn(data_size, input_dim)\n",
    "    labels = torch.randn(data_size, output_dim)\n",
    "\n",
    "    dataset = TensorDataset(input_data, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = SimpleNet(input_dim, hidden_dim, output_dim)\n",
    "    # model.to(device)\n",
    "    \n",
    "    # deepspeed_plugin = DeepSpeedPlugin(zero_stage=2, gradient_clipping=1.0)\n",
    "    # accelerator = Accelerator(deepspeed_plugin=deepspeed_plugin)\n",
    "    accelerator = Accelerator()\n",
    "    optimization = torch.optim.Adam(model.parameters(), lr=0.00015)\n",
    "    crition = torch.nn.MSELoss()\n",
    "    \n",
    "    model, dataloader, optimization = accelerator.prepare(model, dataloader, optimization)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        for batch in dataloader:\n",
    "            inputs, labels = batch\n",
    "            # inputs = inputs.to(device)\n",
    "            # labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = crition(outputs, labels)\n",
    "            \n",
    "            optimization.zero_grad()\n",
    "            # loss.backward()\n",
    "            accelerator.backward(loss)\n",
    "            optimization.step()\n",
    "        print(f\"Epoch {epoch} loss: {loss.item()}\")\n",
    "\n",
    "    end_time = time.time()  # 记录训练结束时间\n",
    "    training_time = end_time - start_time  # 计算训练时间\n",
    "    print(f\"Training time: {training_time:.2f} seconds\")\n",
    "\n",
    "    # accelerator.save(model.state_dict(), \"model.pth\")\n",
    "\n",
    "notebook_launcher(main, num_processes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a22b88",
   "metadata": {},
   "source": [
    "运行请参考同目录下的shell脚本，下面为双卡运行的结果，两个损失独立计算，但是尽管每个 GPU 的损失是独立计算的，但梯度会在所有 GPU 之间通过通信（如 All-Reduce）进行同步，从而保证模型参数的一致性。\n",
    "\n",
    "存在问题：  \n",
    "1、损失会打印两遍，可以将两个损失加起来再求平均；或者只打印主进程的损失\n",
    "\n",
    "\n",
    "Epoch 0 loss: 1.222244143486023\n",
    "Epoch 0 loss: 1.1932578086853027\n",
    "Epoch 1 loss: 0.9584339261054993\n",
    "Epoch 1 loss: 1.3205852508544922\n",
    "Epoch 2 loss: 1.0940974950790405\n",
    "Epoch 2 loss: 0.9358994960784912\n",
    "Epoch 3 loss: 1.112207055091858\n",
    "Epoch 3 loss: 1.2235267162322998\n",
    "Epoch 4 loss: 1.1149256229400635\n",
    "Epoch 4 loss: 1.2418652772903442\n",
    "Epoch 5 loss: 0.9817483425140381\n",
    "Epoch 5 loss: 1.0360157489776611\n",
    "Epoch 6 loss: 1.0470519065856934\n",
    "Epoch 6 loss: 1.1236121654510498\n",
    "Epoch 7 loss: 0.9477889537811279\n",
    "Epoch 7 loss: 1.0988385677337646\n",
    "Epoch 8 loss: 0.9655306935310364\n",
    "Epoch 8 loss: 0.9430689215660095\n",
    "Epoch 9 loss: 1.139449119567871\n",
    "Epoch 9 loss: 0.7671375274658203\n",
    "Epoch 10 loss: 1.2914409637451172Epoch 10 loss: 1.020603060722351\n",
    "\n",
    "Epoch 11 loss: 0.9398835897445679\n",
    "Epoch 11 loss: 1.086294412612915\n",
    "Epoch 12 loss: 0.941448986530304\n",
    "Epoch 12 loss: 0.9452714920043945\n",
    "Epoch 13 loss: 1.0518507957458496Epoch 13 loss: 1.0442299842834473\n",
    "\n",
    "Epoch 14 loss: 0.8157416582107544Epoch 14 loss: 0.8653008341789246\n",
    "\n",
    "Epoch 15 loss: 0.93047696352005\n",
    "Epoch 15 loss: 1.0393348932266235\n",
    "Epoch 16 loss: 1.037017822265625\n",
    "Epoch 16 loss: 1.0681655406951904\n",
    "Epoch 17 loss: 0.7890644669532776Epoch 17 loss: 1.1021982431411743\n",
    "\n",
    "Epoch 18 loss: 0.959057629108429\n",
    "Epoch 18 loss: 0.9632008671760559\n",
    "Epoch 19 loss: 1.2127506732940674\n",
    "Epoch 19 loss: 1.1701936721801758\n",
    "Epoch 20 loss: 1.087782859802246\n",
    "Epoch 20 loss: 1.262906789779663\n",
    "Epoch 21 loss: 1.0912950038909912Epoch 21 loss: 1.0063831806182861\n",
    "\n",
    "Epoch 22 loss: 0.8336697816848755Epoch 22 loss: 0.8239030838012695\n",
    "\n",
    "Epoch 23 loss: 1.1087322235107422\n",
    "Epoch 23 loss: 1.1554553508758545\n",
    "Epoch 24 loss: 0.9161040186882019\n",
    "Epoch 24 loss: 0.9664233922958374\n",
    "Epoch 25 loss: 1.050710916519165\n",
    "Epoch 25 loss: 1.0629756450653076\n",
    "Epoch 26 loss: 0.8758817911148071Epoch 26 loss: 0.9303441047668457\n",
    "\n",
    "Epoch 27 loss: 0.9692226052284241\n",
    "Epoch 27 loss: 0.9761572480201721\n",
    "Epoch 28 loss: 1.2455298900604248\n",
    "Epoch 28 loss: 1.2066376209259033\n",
    "Epoch 29 loss: 1.150545597076416\n",
    "Epoch 29 loss: 1.0455403327941895\n",
    "Epoch 30 loss: 1.1949223279953003\n",
    "Epoch 30 loss: 0.8285470008850098\n",
    "Epoch 31 loss: 0.7168357372283936Epoch 31 loss: 1.0651519298553467\n",
    "\n",
    "Epoch 32 loss: 0.9221818447113037\n",
    "Epoch 32 loss: 0.9815448522567749\n",
    "Epoch 33 loss: 0.8430807590484619\n",
    "Epoch 33 loss: 0.9851400852203369\n",
    "Epoch 34 loss: 1.1626412868499756Epoch 34 loss: 0.9436689615249634\n",
    "\n",
    "Epoch 35 loss: 0.9057785868644714\n",
    "Epoch 35 loss: 1.016579031944275\n",
    "Epoch 36 loss: 0.8286615610122681\n",
    "Epoch 36 loss: 1.0037178993225098\n",
    "Epoch 37 loss: 0.7945321798324585\n",
    "Epoch 37 loss: 0.9148077368736267\n",
    "Epoch 38 loss: 1.1119959354400635\n",
    "Epoch 38 loss: 0.9336367845535278\n",
    "Epoch 39 loss: 0.9502816200256348Epoch 39 loss: 0.8942989706993103\n",
    "\n",
    "Epoch 40 loss: 0.8680436015129089\n",
    "Epoch 40 loss: 0.9411708116531372\n",
    "Epoch 41 loss: 0.9502044320106506Epoch 41 loss: 1.1237208843231201\n",
    "\n",
    "Epoch 42 loss: 1.048546552658081\n",
    "Epoch 42 loss: 1.1162359714508057\n",
    "Epoch 43 loss: 1.0711779594421387\n",
    "Epoch 43 loss: 1.075994849205017\n",
    "Epoch 44 loss: 0.9065122008323669Epoch 44 loss: 0.9709441661834717\n",
    "\n",
    "Epoch 45 loss: 0.9690513610839844\n",
    "Epoch 45 loss: 1.080939531326294\n",
    "Epoch 46 loss: 0.9084521532058716\n",
    "Epoch 46 loss: 1.0406299829483032\n",
    "Epoch 47 loss: 0.9158217906951904Epoch 47 loss: 0.9350024461746216\n",
    "\n",
    "Epoch 48 loss: 1.0812666416168213\n",
    "Epoch 48 loss: 1.1399707794189453\n",
    "Epoch 49 loss: 0.9048585295677185\n",
    "Epoch 49 loss: 1.142801284790039\n",
    "Epoch 50 loss: 0.9551778435707092\n",
    "Epoch 50 loss: 1.0305641889572144\n",
    "Epoch 51 loss: 1.110548496246338\n",
    "Epoch 51 loss: 0.8209130764007568\n",
    "Epoch 52 loss: 0.9793894290924072\n",
    "Epoch 52 loss: 1.168177843093872\n",
    "Epoch 53 loss: 1.0132465362548828\n",
    "Epoch 53 loss: 0.8407228589057922\n",
    "Epoch 54 loss: 1.002813696861267\n",
    "Epoch 54 loss: 1.0878620147705078\n",
    "Epoch 55 loss: 1.015448808670044\n",
    "Epoch 55 loss: 1.064071774482727\n",
    "Epoch 56 loss: 1.166907787322998\n",
    "Epoch 56 loss: 1.0066497325897217\n",
    "Epoch 57 loss: 1.1030890941619873\n",
    "Epoch 57 loss: 0.9938492774963379\n",
    "Epoch 58 loss: 0.9750868082046509Epoch 58 loss: 0.9958294630050659\n",
    "\n",
    "Epoch 59 loss: 0.9356696009635925\n",
    "Epoch 59 loss: 1.2875062227249146\n",
    "Epoch 60 loss: 1.1039798259735107\n",
    "Epoch 60 loss: 1.227920413017273\n",
    "Epoch 61 loss: 0.9704052209854126\n",
    "Epoch 61 loss: 0.9694112539291382\n",
    "Epoch 62 loss: 1.0148450136184692\n",
    "Epoch 62 loss: 1.0889440774917603\n",
    "Epoch 63 loss: 1.0066773891448975\n",
    "Epoch 63 loss: 1.2679240703582764\n",
    "Epoch 64 loss: 1.0210902690887451\n",
    "Epoch 64 loss: 1.22023344039917\n",
    "Epoch 65 loss: 0.906128466129303\n",
    "Epoch 65 loss: 1.1522164344787598\n",
    "Epoch 66 loss: 0.8859699964523315Epoch 66 loss: 0.9274510145187378\n",
    "\n",
    "Epoch 67 loss: 1.098000168800354\n",
    "Epoch 67 loss: 1.0197765827178955\n",
    "Epoch 68 loss: 1.0629165172576904\n",
    "Epoch 68 loss: 0.9598166346549988\n",
    "Epoch 69 loss: 1.1767737865447998\n",
    "Epoch 69 loss: 1.03900945186615\n",
    "Epoch 70 loss: 0.9126209020614624Epoch 70 loss: 0.8794825673103333\n",
    "\n",
    "Epoch 71 loss: 0.989602267742157\n",
    "Epoch 71 loss: 1.0115382671356201\n",
    "Epoch 72 loss: 0.9187991619110107\n",
    "Epoch 72 loss: 1.0713469982147217\n",
    "Epoch 73 loss: 0.9748698472976685Epoch 73 loss: 1.0427227020263672\n",
    "\n",
    "Epoch 74 loss: 1.2203468084335327\n",
    "Epoch 74 loss: 0.9615665078163147\n",
    "Epoch 75 loss: 0.7661041021347046\n",
    "Epoch 75 loss: 0.9157580137252808\n",
    "Epoch 76 loss: 0.9729267954826355\n",
    "Epoch 76 loss: 0.9903103113174438\n",
    "Epoch 77 loss: 1.032605767250061\n",
    "Epoch 77 loss: 1.1825000047683716\n",
    "Epoch 78 loss: 1.0940577983856201\n",
    "Epoch 78 loss: 0.9936052560806274\n",
    "Epoch 79 loss: 0.9351733922958374\n",
    "Epoch 79 loss: 1.0008231401443481\n",
    "Epoch 80 loss: 0.9143375158309937\n",
    "Epoch 80 loss: 1.066443920135498\n",
    "Epoch 81 loss: 1.0062150955200195Epoch 81 loss: 1.0447568893432617\n",
    "\n",
    "Epoch 82 loss: 0.9024689197540283\n",
    "Epoch 82 loss: 1.1246912479400635\n",
    "Epoch 83 loss: 1.2535226345062256\n",
    "Epoch 83 loss: 0.933487594127655\n",
    "Epoch 84 loss: 0.99973464012146\n",
    "Epoch 84 loss: 0.9624868631362915\n",
    "Epoch 85 loss: 1.0391117334365845Epoch 85 loss: 1.2220211029052734\n",
    "\n",
    "Epoch 86 loss: 0.8270946741104126Epoch 86 loss: 1.037883996963501\n",
    "\n",
    "Epoch 87 loss: 1.0317413806915283\n",
    "Epoch 87 loss: 1.0438865423202515\n",
    "Epoch 88 loss: 0.8812784552574158Epoch 88 loss: 1.0950701236724854\n",
    "\n",
    "Epoch 89 loss: 0.8820447325706482\n",
    "Epoch 89 loss: 0.9369880557060242\n",
    "Epoch 90 loss: 0.9372445344924927Epoch 90 loss: 1.0238240957260132\n",
    "\n",
    "Epoch 91 loss: 1.058774709701538Epoch 91 loss: 0.9712309837341309\n",
    "\n",
    "Epoch 92 loss: 1.00765061378479\n",
    "Epoch 92 loss: 1.0069276094436646\n",
    "Epoch 93 loss: 1.2989996671676636Epoch 93 loss: 1.1104843616485596\n",
    "\n",
    "Epoch 94 loss: 0.867250382900238Epoch 94 loss: 0.8564057350158691\n",
    "\n",
    "Epoch 95 loss: 1.0129553079605103\n",
    "Epoch 95 loss: 1.141326665878296\n",
    "Epoch 96 loss: 1.204880714416504Epoch 96 loss: 0.9655894637107849\n",
    "\n",
    "Epoch 97 loss: 0.9894803166389465Epoch 97 loss: 1.0592126846313477\n",
    "\n",
    "Epoch 98 loss: 0.9539648294448853Epoch 98 loss: 0.9008721113204956\n",
    "\n",
    "Epoch 99 loss: 1.0199759006500244\n",
    "Training time: 29.51 seconds\n",
    "Epoch 99 loss: 1.078111171722412\n",
    "Training time: 29.51 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d89f096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 聚合一下损失，并且只输出主进程的结果\n",
    "\n",
    "'''\n",
    "单卡直接运行：35.76 seconds\n",
    "单卡accelerate：48.82 seconds\n",
    "双卡accelerate：29.51 seconds\n",
    "'''\n",
    "\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import time\n",
    "\n",
    "class SimpleNet(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)  #.to(\"cuda:0\")\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, output_dim) #.to(\"cuda:1\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.to(\"cuda:0\")\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        # x.to(\"cuda:1\")\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_dim = 10\n",
    "    hidden_dim = 20\n",
    "    output_dim = 2\n",
    "    batch_size = 64\n",
    "    data_size = 10000\n",
    "\n",
    "    device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    input_data = torch.randn(data_size, input_dim)\n",
    "    labels = torch.randn(data_size, output_dim)\n",
    "\n",
    "    dataset = TensorDataset(input_data, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = SimpleNet(input_dim, hidden_dim, output_dim)\n",
    "    # model.to(device)\n",
    "    \n",
    "    # deepspeed_plugin = DeepSpeedPlugin(zero_stage=2, gradient_clipping=1.0)\n",
    "    # accelerator = Accelerator(deepspeed_plugin=deepspeed_plugin)\n",
    "    accelerator = Accelerator()\n",
    "    optimization = torch.optim.Adam(model.parameters(), lr=0.00015)\n",
    "    crition = torch.nn.MSELoss()\n",
    "    # print(f'len(dataloader):{len(dataloader)}')\n",
    "    model, dataloader, optimization = accelerator.prepare(model, dataloader, optimization)\n",
    "    # print(f'len(dataloader):{len(dataloader)}')\n",
    "    start_time = time.time()\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        # total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            inputs, labels = batch\n",
    "            # inputs = inputs.to(device)\n",
    "            # labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = crition(outputs, labels)\n",
    "            # total_loss += loss.item()\n",
    "            optimization.zero_grad()\n",
    "            # loss.backward()\n",
    "            accelerator.backward(loss)\n",
    "            optimization.step()\n",
    "        \n",
    "        # 这里的loss只是取了最后一次的loss，并不是所有batch的loss\n",
    "\n",
    "        # 下面两个方法得到的结果是一样的\n",
    "        gather_loss = accelerator.gather(loss).mean()  # 收集所有 GPU 的损失并取平均\n",
    "        if accelerator.is_main_process:\n",
    "            print(f\"Epoch {epoch} loss: {gather_loss.item()}\")\n",
    "\n",
    "        # avg_loss = accelerator.reduce(loss, reduction=\"mean\")\n",
    "        # if accelerator.is_main_process:\n",
    "        #     print(f\"Epoch {epoch} loss: {avg_loss.item()}\")\n",
    "\n",
    "    end_time = time.time()  # 记录训练结束时间\n",
    "    training_time = end_time - start_time  # 计算训练时间\n",
    "    if accelerator.is_main_process:\n",
    "        print(f\"Training time: {training_time:.2f} seconds\")\n",
    "\n",
    "    # accelerator.save(model.state_dict(), \"model.pth\")\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdae93f5",
   "metadata": {},
   "source": [
    "Epoch 0 loss: 0.9432018399238586\n",
    "Epoch 1 loss: 0.9678932428359985\n",
    "Epoch 2 loss: 1.0469319820404053\n",
    "Epoch 3 loss: 1.0713804960250854\n",
    "Epoch 4 loss: 0.9084330797195435\n",
    "Epoch 5 loss: 0.916047215461731\n",
    "Epoch 6 loss: 1.0488253831863403\n",
    "Epoch 7 loss: 1.066176176071167\n",
    "Epoch 8 loss: 1.2106516361236572\n",
    "Epoch 9 loss: 0.8363816738128662\n",
    "Epoch 10 loss: 1.0502277612686157\n",
    "Epoch 11 loss: 1.1638139486312866\n",
    "Epoch 12 loss: 0.9493512511253357\n",
    "Epoch 13 loss: 1.017587661743164\n",
    "Epoch 14 loss: 0.9472472667694092\n",
    "Epoch 15 loss: 0.9615411162376404\n",
    "Epoch 16 loss: 0.990830659866333\n",
    "Epoch 17 loss: 0.8506342172622681\n",
    "Epoch 18 loss: 1.0821845531463623\n",
    "Epoch 19 loss: 1.1170412302017212\n",
    "Epoch 20 loss: 1.044736623764038\n",
    "Epoch 21 loss: 0.858221709728241\n",
    "Epoch 22 loss: 0.8285454511642456\n",
    "Epoch 23 loss: 0.8669906258583069\n",
    "Epoch 24 loss: 1.1118721961975098\n",
    "Epoch 25 loss: 1.1339545249938965\n",
    "Epoch 26 loss: 1.152949571609497\n",
    "Epoch 27 loss: 0.9197186827659607\n",
    "Epoch 28 loss: 1.0091302394866943\n",
    "Epoch 29 loss: 1.1065753698349\n",
    "Epoch 30 loss: 0.9354009032249451\n",
    "Epoch 31 loss: 0.9681801795959473\n",
    "Epoch 32 loss: 0.934136152267456\n",
    "Epoch 33 loss: 1.1814976930618286\n",
    "Epoch 34 loss: 1.1219456195831299\n",
    "Epoch 35 loss: 0.9535113573074341\n",
    "Epoch 36 loss: 1.0602713823318481\n",
    "Epoch 37 loss: 0.8352175951004028\n",
    "Epoch 38 loss: 0.837505578994751\n",
    "Epoch 39 loss: 0.9966219067573547\n",
    "Epoch 40 loss: 0.9990330338478088\n",
    "Epoch 41 loss: 0.902934193611145\n",
    "Epoch 42 loss: 0.9071310758590698\n",
    "Epoch 43 loss: 0.9718709588050842\n",
    "Epoch 44 loss: 1.1083896160125732\n",
    "Epoch 45 loss: 1.0436538457870483\n",
    "Epoch 46 loss: 0.9891265630722046\n",
    "Epoch 47 loss: 1.0118757486343384\n",
    "Epoch 48 loss: 1.0493839979171753\n",
    "Epoch 49 loss: 0.9719559550285339\n",
    "Epoch 50 loss: 0.9746475219726562\n",
    "Epoch 51 loss: 0.8756150603294373\n",
    "Epoch 52 loss: 1.1143317222595215\n",
    "Epoch 53 loss: 1.0076954364776611\n",
    "Epoch 54 loss: 1.0678539276123047\n",
    "Epoch 55 loss: 1.098946452140808\n",
    "Epoch 56 loss: 1.0479623079299927\n",
    "Epoch 57 loss: 0.8356451988220215\n",
    "Epoch 58 loss: 1.066871166229248\n",
    "Epoch 59 loss: 1.000270128250122\n",
    "Epoch 60 loss: 1.0957651138305664\n",
    "Epoch 61 loss: 0.9438618421554565\n",
    "Epoch 62 loss: 1.0931410789489746\n",
    "Epoch 63 loss: 0.8018264770507812\n",
    "Epoch 64 loss: 0.8660523891448975\n",
    "Epoch 65 loss: 1.01133131980896\n",
    "Epoch 66 loss: 0.9543184041976929\n",
    "Epoch 67 loss: 0.9269616603851318\n",
    "Epoch 68 loss: 0.9933525323867798\n",
    "Epoch 69 loss: 1.0068784952163696\n",
    "Epoch 70 loss: 1.072657823562622\n",
    "Epoch 71 loss: 1.026125192642212\n",
    "Epoch 72 loss: 0.9402647614479065\n",
    "Epoch 73 loss: 1.0038683414459229\n",
    "Epoch 74 loss: 0.8630919456481934\n",
    "Epoch 75 loss: 1.0004982948303223\n",
    "Epoch 76 loss: 0.9028060436248779\n",
    "Epoch 77 loss: 1.135162353515625\n",
    "Epoch 78 loss: 1.0471564531326294\n",
    "Epoch 79 loss: 1.036384105682373\n",
    "Epoch 80 loss: 0.9773803949356079\n",
    "Epoch 81 loss: 0.8492220640182495\n",
    "Epoch 82 loss: 1.0323593616485596\n",
    "Epoch 83 loss: 1.0219027996063232\n",
    "Epoch 84 loss: 1.177842140197754\n",
    "Epoch 85 loss: 1.1461409330368042\n",
    "Epoch 86 loss: 0.9765742421150208\n",
    "Epoch 87 loss: 0.9133744239807129\n",
    "Epoch 88 loss: 0.8916490077972412\n",
    "Epoch 89 loss: 0.9485730528831482\n",
    "Epoch 90 loss: 1.1236549615859985\n",
    "Epoch 91 loss: 0.9537147283554077\n",
    "Epoch 92 loss: 0.9323829412460327\n",
    "Epoch 93 loss: 0.8480007648468018\n",
    "Epoch 94 loss: 0.922979474067688\n",
    "Epoch 95 loss: 0.8562111854553223\n",
    "Epoch 96 loss: 0.8881111145019531\n",
    "Epoch 97 loss: 1.0885865688323975\n",
    "Epoch 98 loss: 1.0496211051940918\n",
    "Epoch 99 loss: 1.0826056003570557\n",
    "Training time: 33.32 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb50b1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用deepseed\n",
    "\n",
    "'''\n",
    "单卡直接运行：35.76 seconds\n",
    "单卡accelerate：48.82 seconds\n",
    "双卡accelerate：29.51 seconds\n",
    "'''\n",
    "\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import time\n",
    "\n",
    "class SimpleNet(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)  #.to(\"cuda:0\")\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, output_dim) #.to(\"cuda:1\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.to(\"cuda:0\")\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        # x.to(\"cuda:1\")\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_dim = 10\n",
    "    hidden_dim = 20\n",
    "    output_dim = 2\n",
    "    batch_size = 64\n",
    "    data_size = 10000\n",
    "\n",
    "    device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    input_data = torch.randn(data_size, input_dim)\n",
    "    labels = torch.randn(data_size, output_dim)\n",
    "\n",
    "    dataset = TensorDataset(input_data, labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = SimpleNet(input_dim, hidden_dim, output_dim)\n",
    "    # model.to(device)\n",
    "    \n",
    "    deepspeed_plugin = DeepSpeedPlugin(zero_stage=2, gradient_clipping=1.0)\n",
    "    accelerator = Accelerator(deepspeed_plugin=deepspeed_plugin)\n",
    "    # accelerator = Accelerator()\n",
    "    optimization = torch.optim.Adam(model.parameters(), lr=0.00015)\n",
    "    crition = torch.nn.MSELoss()\n",
    "    # print(f'len(dataloader):{len(dataloader)}')\n",
    "    model, dataloader, optimization = accelerator.prepare(model, dataloader, optimization)\n",
    "    # print(f'len(dataloader):{len(dataloader)}')\n",
    "    start_time = time.time()\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        # total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            inputs, labels = batch\n",
    "            # inputs = inputs.to(device)\n",
    "            # labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = crition(outputs, labels)\n",
    "            # total_loss += loss.item()\n",
    "            optimization.zero_grad()\n",
    "            # loss.backward()\n",
    "            accelerator.backward(loss)\n",
    "            optimization.step()\n",
    "        \n",
    "        # 这里的loss只是取了最后一次的loss，并不是所有batch的loss\n",
    "\n",
    "        # 下面两个方法得到的结果是一样的\n",
    "        gather_loss = accelerator.gather(loss).mean()  # 收集所有 GPU 的损失并取平均\n",
    "        if accelerator.is_main_process:\n",
    "            print(f\"Epoch {epoch} loss: {gather_loss.item()}\")\n",
    "\n",
    "        # avg_loss = accelerator.reduce(loss, reduction=\"mean\")\n",
    "        # if accelerator.is_main_process:\n",
    "        #     print(f\"Epoch {epoch} loss: {avg_loss.item()}\")\n",
    "\n",
    "    end_time = time.time()  # 记录训练结束时间\n",
    "    training_time = end_time - start_time  # 计算训练时间\n",
    "    if accelerator.is_main_process:\n",
    "        print(f\"Training time: {training_time:.2f} seconds\")\n",
    "\n",
    "    # accelerator.save(model.state_dict(), \"model.pth\")\n",
    "            \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseek2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
