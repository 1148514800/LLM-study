{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、注意力机制简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在注意力机制中，Q 、K 、V 分别代表 Query（查询） 、Key（键） 和 Value（值）  \n",
    "1、Q 是用来表示当前需要关注的内容或查询的目标。可以将其理解为一个“问题”或“需求”，它用于与其他内容进行匹配，以找到相关的信息；  \n",
    "2、K 是用来表示被查询的内容或可供匹配的信息。可以将其理解为一个“标识符”或“索引”，它与 Q 进行相似度计算，从而判断两者的相关性； \n",
    "3、V 是实际的内容或信息载体，包含了具体的语义信息。当 Q 和 K 计算出相似度后，通过加权求和的方式从 V 中提取相关信息。  \n",
    "  \n",
    "Q 和 K 的点积结果 被称为“注意力分数”（attention score），它表示当前查询（Query）对某个键（Key）的关注程度，注意力分数越高，说明该键对应的值（Value）对当前查询更重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、自注意力机制简单示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "B, T, C = 4, 8, 2\n",
    "v = torch.randn(B, T, C)\n",
    "\n",
    "tril = torch.tril(torch.ones((T, T)))\n",
    "wei = torch.zeros((T, T))\n",
    "wei_mask = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei_softmax = F.softmax(wei_mask, dim=-1)\n",
    "out = wei_softmax @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1.]]),\n",
       " torch.Size([8, 8]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril, tril.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       " torch.Size([8, 8]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei_mask, wei_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "         [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "         [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]]),\n",
       " torch.Size([8, 8]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei_softmax, wei_softmax.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "out = wei_softmax @ v：(B, T, T) @ (B, T, C) -> (B, T, C)  \n",
    "完成的效果即为对过去的信息进行加权求和，得到当前时刻的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "print(v[0][0][0] == out[0][0][0])  # True\n",
    "print((v[0][0][0] + v[0][1][0]) / 2 == out[0][1][0])   # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.2549, -0.3037],\n",
       "         [-0.4198,  1.5528],\n",
       "         [ 1.0640, -0.3529],\n",
       "         [-1.7614,  0.2654],\n",
       "         [ 1.2936, -1.0602],\n",
       "         [-1.3749, -0.0089],\n",
       "         [ 1.3363, -0.9032],\n",
       "         [ 0.3539, -1.3199]]),\n",
       " tensor([[ 1.2549, -0.3037],\n",
       "         [ 0.4175,  0.6245],\n",
       "         [ 0.6330,  0.2987],\n",
       "         [ 0.0344,  0.2904],\n",
       "         [ 0.2863,  0.0203],\n",
       "         [ 0.0094,  0.0154],\n",
       "         [ 0.1989, -0.1158],\n",
       "         [ 0.2183, -0.2663]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[0], out[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、自注意力机制在LLM中的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 4])\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024   # 句子长度\n",
    "    n_head: int = 12    # 注意力头数\n",
    "    n_embd: int = 768   # 每个词的向量维度，即隐藏层维度\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        '''\n",
    "        att即相当于第一节当中的wei_softmax，只不过初始信息为q与k的相乘\n",
    "        '''\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "\n",
    "        # 使用该行代替上面的注意力求解过程的四行，即可应用flash-attention加速\n",
    "        # y = F.scaled_dot_product_attention(q, k, v,is_causal=True)\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    config = GPTConfig(block_size=8, n_head=2, n_embd=4)\n",
    "    model = CausalSelfAttention(config)\n",
    "    x = torch.rand(2, 8, 4)\n",
    "    y = model(x)\n",
    "    print(y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseek2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
